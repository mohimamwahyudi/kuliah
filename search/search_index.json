{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"selamat datang di halaman tugas penambangan data \u00b6 profil \u00b6 `NAMA : MOH. IMAM WAHYUDI `NIM :180411100007 `KELAS :PENAMBANGAN DATA 5-D `JURUSAN : TEKNIK INFORMATIKA \u200b","title":"index"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data","text":"","title":"selamat datang di halaman tugas penambangan data"},{"location":"#profil","text":"`NAMA : MOH. IMAM WAHYUDI `NIM :180411100007 `KELAS :PENAMBANGAN DATA 5-D `JURUSAN : TEKNIK INFORMATIKA \u200b","title":"profil"},{"location":"DECISION TREE/","text":"DECISION TREE \u00b6 decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S berikut contoh data yang akan dinrubah mrnjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 berikut cara membuat decision tree dengan code lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"GENDER\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"CAR TIPE\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"SHIRT SIZE\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"CLASS\" ]) df . style . hide_index () df CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS gender_n car_type_n shirt_size_n class_n 0 1 M FAMILY SMALL C0 1 0 3 0 1 2 M SPORT MEDIUM C0 1 3 2 0 2 3 M SPORT MEDIUM C0 1 3 2 0 3 4 M SPORT LARGE C0 1 3 1 0 4 5 M SPORT EXTRA LARGE C0 1 3 0 0 5 6 M SPORT EXTRA LARGE C0 1 3 0 0 6 7 F SPORT SMALL C0 0 3 3 0 7 8 F SPORT SMALL C0 0 3 3 0 8 9 F SPORT MEDIUM C1 0 3 2 1 9 10 F LUXURY LARGE C1 0 2 1 1 10 11 M FAMILY LARGE C1 1 0 1 1 11 12 M FAMILY EXTRA LARGE C1 1 0 0 1 12 13 M FAMILY MEDIUM C1 1 0 2 1 13 14 M LUCURY EXTRA LARGE C1 1 1 0 1 14 15 F LUCURY SMALL C1 0 1 3 1 15 16 F LUCURY SMALL C1 0 1 3 1 16 17 F LUCURY MEDIUM C1 0 1 2 1 17 18 F LUCURY MEDIUM C1 0 1 2 1 18 19 F LUCURY MEDIUM C1 0 1 2 1 19 20 F LUCURY LARGE C1 0 1 1 1 inputs = df . drop ([ \"CASTEMER ID\" , \"GENDER\" , \"CAR TIPE\" , \"SHIRT SIZE\" , \"CLASS\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier ( class_weight = None , criterion = 'entropy' , max_depth = None , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , min_samples_leaf = 1 , min_samples_split = 2 , min_weight_fraction_leaf = 0.0 , presort = False , random_state = 100 , splitter = 'best' ) from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) [Text(167.4, 195.696, 'node #0\\nGender < = 1.5\\nentropy = 1.0\\nsamples = 100.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(125.55000000000001, 152.208, 'node #1\\nCar Type < = 0.5\\nentropy = 0.65\\nsamples = 60.0%\\nvalue = [0.167, 0.833]\\nclass = C1'), Text(83.7, 108.72, 'node #2\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(167.4, 108.72, 'node #3\\nGender < = 0.5\\nentropy = 0.722\\nsamples = 50.0%\\nvalue = [0.2, 0.8]\\nclass = C1'), Text(83.7, 65.232, 'node #4\\nCar Type < = 2.5\\nentropy = 0.918\\nsamples = 15.0%\\nvalue = [0.333, 0.667]\\nclass = C1'), Text(41.85, 21.744, 'node #5\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(125.55000000000001, 21.744, 'node #6\\nentropy = 0.0\\nsamples = 5.0%\\nvalue = [1.0, 0.0]\\nclass = C0'), Text(251.10000000000002, 65.232, 'node #7\\nCar Type < = 1.5\\nentropy = 0.592\\nsamples = 35.0%\\nvalue = [0.143, 0.857]\\nclass = C1'), Text(209.25, 21.744, 'node #8\\nentropy = 1.0\\nsamples = 10.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(292.95, 21.744, 'node #9\\nentropy = 0.0\\nsamples = 25.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(209.25, 152.208, 'node #10\\nentropy = 0.0\\nsamples = 40.0%\\nvalue = [1.0, 0.0]\\nclass = C0')] MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"DECISION TREE"},{"location":"DECISION TREE/#decision-tree","text":"decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE"},{"location":"DECISION TREE/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S berikut contoh data yang akan dinrubah mrnjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 berikut cara membuat decision tree dengan code lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"GENDER\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"CAR TIPE\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"SHIRT SIZE\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"CLASS\" ]) df . style . hide_index () df CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS gender_n car_type_n shirt_size_n class_n 0 1 M FAMILY SMALL C0 1 0 3 0 1 2 M SPORT MEDIUM C0 1 3 2 0 2 3 M SPORT MEDIUM C0 1 3 2 0 3 4 M SPORT LARGE C0 1 3 1 0 4 5 M SPORT EXTRA LARGE C0 1 3 0 0 5 6 M SPORT EXTRA LARGE C0 1 3 0 0 6 7 F SPORT SMALL C0 0 3 3 0 7 8 F SPORT SMALL C0 0 3 3 0 8 9 F SPORT MEDIUM C1 0 3 2 1 9 10 F LUXURY LARGE C1 0 2 1 1 10 11 M FAMILY LARGE C1 1 0 1 1 11 12 M FAMILY EXTRA LARGE C1 1 0 0 1 12 13 M FAMILY MEDIUM C1 1 0 2 1 13 14 M LUCURY EXTRA LARGE C1 1 1 0 1 14 15 F LUCURY SMALL C1 0 1 3 1 15 16 F LUCURY SMALL C1 0 1 3 1 16 17 F LUCURY MEDIUM C1 0 1 2 1 17 18 F LUCURY MEDIUM C1 0 1 2 1 18 19 F LUCURY MEDIUM C1 0 1 2 1 19 20 F LUCURY LARGE C1 0 1 1 1 inputs = df . drop ([ \"CASTEMER ID\" , \"GENDER\" , \"CAR TIPE\" , \"SHIRT SIZE\" , \"CLASS\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier ( class_weight = None , criterion = 'entropy' , max_depth = None , max_features = None , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , min_samples_leaf = 1 , min_samples_split = 2 , min_weight_fraction_leaf = 0.0 , presort = False , random_state = 100 , splitter = 'best' ) from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) [Text(167.4, 195.696, 'node #0\\nGender < = 1.5\\nentropy = 1.0\\nsamples = 100.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(125.55000000000001, 152.208, 'node #1\\nCar Type < = 0.5\\nentropy = 0.65\\nsamples = 60.0%\\nvalue = [0.167, 0.833]\\nclass = C1'), Text(83.7, 108.72, 'node #2\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(167.4, 108.72, 'node #3\\nGender < = 0.5\\nentropy = 0.722\\nsamples = 50.0%\\nvalue = [0.2, 0.8]\\nclass = C1'), Text(83.7, 65.232, 'node #4\\nCar Type < = 2.5\\nentropy = 0.918\\nsamples = 15.0%\\nvalue = [0.333, 0.667]\\nclass = C1'), Text(41.85, 21.744, 'node #5\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(125.55000000000001, 21.744, 'node #6\\nentropy = 0.0\\nsamples = 5.0%\\nvalue = [1.0, 0.0]\\nclass = C0'), Text(251.10000000000002, 65.232, 'node #7\\nCar Type < = 1.5\\nentropy = 0.592\\nsamples = 35.0%\\nvalue = [0.143, 0.857]\\nclass = C1'), Text(209.25, 21.744, 'node #8\\nentropy = 1.0\\nsamples = 10.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(292.95, 21.744, 'node #9\\nentropy = 0.0\\nsamples = 25.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(209.25, 152.208, 'node #10\\nentropy = 0.0\\nsamples = 40.0%\\nvalue = [1.0, 0.0]\\nclass = C0')] MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"CARA MEMBUAT DECISION TREE"},{"location":"MISSING VELUES/","text":"jarak \u00b6 Menghitung jarak tipe numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Manhattan distance $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Euclidean distance Average Distance $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Weighted euclidean distance $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Chord distance $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ ##### Menghitung Jarak Ordinal Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf rumus $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ ##### Menghitung jarak Binary Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+t $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ berikut menghitung jarak dengan python ~~~py import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder data = pd.read_csv('jarak.csv', sep=';') df = pd.DataFrame(data) df.style.hide_index() ~~~ nama jesnis kelamin ipk p ortu prestasi imam l 3.8 3000000 internasional suci p 3.5 9000000 nasional rizky l 3.3 4000000 regional ```py X = data.iloc[:,:].values labelEncode_X = LabelEncoder() X[:,1] = labelEncode_X.fit_transform(X[:,1]) def Zscore(x,mean,std): top = x - mean if top==0: return top else: return round(top / std, 2) #menghitung jarak tipe numerikal def euclidianDistance(x,y): dis = 0 for i in range(len(x)): dis += (x[i] - y[i]) ** 2 return round(mt.sqrt(dis),2) def normalisasi(num, col_x): return Zscore(num, pd.Series(data[col_x].values).mean(), pd.Series(data[col_x].values).std()) #Menghitung Jarak tipe categorikal def distanceNom(x,y): p = len(x) or len(y) m = 0 for i in range(len(x)): if x[i][0] == y[i][0]: m +=1 return (p - m) / p #Menghitung Jarak tipe ordinal #inisialisasi x = {'Internasional':3,'Nasional':2,'Regional':1} def normalizedOrd(y): i_max = 0 for i in x: if x[i] > i_max: i_max = x[i] if y[0] == i: i_val = x[i] return (i_val - 1) / (i_max - 1) #Menghitung jarak tipe binary def distanceSimetris(x,y): q=r=s=t=0 for i in range(len(x)): if x[i]==1 and y[i]==1: q+=1 elif x[i]==1 and y[i]==0: r+=1 elif x[i]==0 and y[i]==1: s+=1 elif x[i]==0 and y[i]==0: t+=1 return ((r+s)/(q+r+s+t)) d_x = { 0 : ['', 'Ali', 'Ani', 'Abi'], 1 : ['Ali', 0, '', ''], 2 : ['Ani', '', 0, ''], 3 : ['Abi', '', '', 0] } #ambil data numerikal imamNum = df.iloc[0, 2:4].values suciNum = df.iloc[1, 2:4].values rizkyNum = df.iloc[2, 2:4].values #normalisasi data numerikal imamNum = [normalisasi(aliNum[0], data.columns[2]), normalisasi(aliNum[1], data.columns[3])] suciNum = [normalisasi(aniNum[0], data.columns[2]), normalisasi(aniNum[1], data.columns[3])] suciNum = [normalisasi(abiNum[0], data.columns[2]), normalisasi(abiNum[1], data.columns[3])] d_x[1][2] = euclidianDistance(aniNum,aliNum) d_x[1][3] = euclidianDistance(abiNum,aliNum) d_x[2][3] = euclidianDistance(abiNum,aniNum) d_x = pd.DataFrame(d_x) d_x.style.hide_index() ``` 0 1 2 3 imam suci rizky imam 0 suci 2.83 0 rizky 1.41 1.41 0 #ambil data binary imamBin = X [ 0 , 1 : 2 ] suciBin = X [ 1 , 1 : 2 ] rizkyBin = X [ 2 , 1 : 2 ] d_x [ 1 ][ 2 ] = distanceSimetris ( suciBin , imamBin ) d_x [ 1 ][ 3 ] = distanceSimetris ( rizkyBin , imamBin ) d_x [ 2 ][ 3 ] = distanceSimetris ( rizkyBin , suciBin ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 imam suci rizky imam 0 suci 1 0 rizky 0 1 0 #ambil data ordinal aliOrd = [ df . iloc [ 0 , 5 : 6 ] . values ] aniOrd = [ df . iloc [ 1 , 5 : 6 ] . values ] abiOrd = [ df . iloc [ 2 , 5 : 6 ] . values ] d_x [ 1 ][ 2 ] = euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 imam suci rizky imam 0 suci 1 0 rizky 0.5 0.5 0 #ambil data ordinal aliOrd = [ df . iloc [ 0 , 5 : 6 ] . values ] aniOrd = [ df . iloc [ 1 , 5 : 6 ] . values ] abiOrd = [ df . iloc [ 2 , 5 : 6 ] . values ] d_x [ 1 ][ 2 ] = euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 imam suci imam imam 0 suci 5.83 0 rizky 2.91 3.91 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"jarak"},{"location":"MISSING VELUES/#jarak","text":"","title":"jarak"},{"location":"MISSING VELUES/#menghitung-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Manhattan distance $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Euclidean distance Average Distance $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Weighted euclidean distance $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Chord distance $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ ##### Menghitung Jarak Ordinal Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf rumus $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ ##### Menghitung jarak Binary Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+t $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ berikut menghitung jarak dengan python ~~~py import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder data = pd.read_csv('jarak.csv', sep=';') df = pd.DataFrame(data) df.style.hide_index() ~~~ nama jesnis kelamin ipk p ortu prestasi imam l 3.8 3000000 internasional suci p 3.5 9000000 nasional rizky l 3.3 4000000 regional ```py X = data.iloc[:,:].values labelEncode_X = LabelEncoder() X[:,1] = labelEncode_X.fit_transform(X[:,1]) def Zscore(x,mean,std): top = x - mean if top==0: return top else: return round(top / std, 2) #menghitung jarak tipe numerikal def euclidianDistance(x,y): dis = 0 for i in range(len(x)): dis += (x[i] - y[i]) ** 2 return round(mt.sqrt(dis),2) def normalisasi(num, col_x): return Zscore(num, pd.Series(data[col_x].values).mean(), pd.Series(data[col_x].values).std()) #Menghitung Jarak tipe categorikal def distanceNom(x,y): p = len(x) or len(y) m = 0 for i in range(len(x)): if x[i][0] == y[i][0]: m +=1 return (p - m) / p #Menghitung Jarak tipe ordinal #inisialisasi x = {'Internasional':3,'Nasional':2,'Regional':1} def normalizedOrd(y): i_max = 0 for i in x: if x[i] > i_max: i_max = x[i] if y[0] == i: i_val = x[i] return (i_val - 1) / (i_max - 1) #Menghitung jarak tipe binary def distanceSimetris(x,y): q=r=s=t=0 for i in range(len(x)): if x[i]==1 and y[i]==1: q+=1 elif x[i]==1 and y[i]==0: r+=1 elif x[i]==0 and y[i]==1: s+=1 elif x[i]==0 and y[i]==0: t+=1 return ((r+s)/(q+r+s+t)) d_x = { 0 : ['', 'Ali', 'Ani', 'Abi'], 1 : ['Ali', 0, '', ''], 2 : ['Ani', '', 0, ''], 3 : ['Abi', '', '', 0] } #ambil data numerikal imamNum = df.iloc[0, 2:4].values suciNum = df.iloc[1, 2:4].values rizkyNum = df.iloc[2, 2:4].values #normalisasi data numerikal imamNum = [normalisasi(aliNum[0], data.columns[2]), normalisasi(aliNum[1], data.columns[3])] suciNum = [normalisasi(aniNum[0], data.columns[2]), normalisasi(aniNum[1], data.columns[3])] suciNum = [normalisasi(abiNum[0], data.columns[2]), normalisasi(abiNum[1], data.columns[3])] d_x[1][2] = euclidianDistance(aniNum,aliNum) d_x[1][3] = euclidianDistance(abiNum,aliNum) d_x[2][3] = euclidianDistance(abiNum,aniNum) d_x = pd.DataFrame(d_x) d_x.style.hide_index() ``` 0 1 2 3 imam suci rizky imam 0 suci 2.83 0 rizky 1.41 1.41 0 #ambil data binary imamBin = X [ 0 , 1 : 2 ] suciBin = X [ 1 , 1 : 2 ] rizkyBin = X [ 2 , 1 : 2 ] d_x [ 1 ][ 2 ] = distanceSimetris ( suciBin , imamBin ) d_x [ 1 ][ 3 ] = distanceSimetris ( rizkyBin , imamBin ) d_x [ 2 ][ 3 ] = distanceSimetris ( rizkyBin , suciBin ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 imam suci rizky imam 0 suci 1 0 rizky 0 1 0 #ambil data ordinal aliOrd = [ df . iloc [ 0 , 5 : 6 ] . values ] aniOrd = [ df . iloc [ 1 , 5 : 6 ] . values ] abiOrd = [ df . iloc [ 2 , 5 : 6 ] . values ] d_x [ 1 ][ 2 ] = euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 imam suci rizky imam 0 suci 1 0 rizky 0.5 0.5 0 #ambil data ordinal aliOrd = [ df . iloc [ 0 , 5 : 6 ] . values ] aniOrd = [ df . iloc [ 1 , 5 : 6 ] . values ] abiOrd = [ df . iloc [ 2 , 5 : 6 ] . values ] d_x [ 1 ][ 2 ] = euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 imam suci imam imam 0 suci 5.83 0 rizky 2.91 3.91 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Menghitung jarak tipe numerik"},{"location":"Untitled8 (1)/","text":"MISSING VELUE \u00b6 cara mengisi data yang hilang adalah dengan menghitung jarak nya terlebih dahulu ,data yang hilang berkemungkinan akan bermasalah pada kemuadian hari. ada dua cara mengatasi missing velue dengan cara mengahapus data yang hilang ketika data tersebut dibawah 5% atau bisa dengan cara knn untuk mengisinyanya kembali ##### berikut langkah dari algoritma K-NN kita harus mengambil 2 tetangga terdekat dengan missing vellue hitung jarak tetangga tadi lalu urutkan dari yang terbesar ke yang terkecil ambil urutan yang terkecil lalu rata rata #### berikut ini cara mengatasi missing velue dengan python import pandas as pd import numpy as np dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} df = pd . DataFrame ( dict ) df . isnull () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 False False True 1 False False False 2 True False False 3 False True False # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # creating bool series True for NaN values bool_series = pd . isnull ( data [ \"Gender\" ]) # filtering data # displaying data only with Gender = NaN data [ bool_series ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 20 Lois NaN 4/22/1995 7:18 PM 64714 4.934 True Legal 22 Joshua NaN 3/8/2012 1:58 AM 90816 18.816 True Client Services 27 Scott NaN 7/11/1991 6:58 PM 122367 5.218 False Legal 31 Joyce NaN 2/20/2005 2:40 PM 88657 12.752 False Product 41 Christine NaN 6/28/2015 1:08 AM 66582 11.308 True Business Development 49 Chris NaN 1/24/1980 12:13 PM 113590 3.055 False Sales 51 NaN NaN 12/17/2011 8:29 AM 41126 14.009 NaN Sales 53 Alan NaN 3/3/2014 1:28 PM 40341 17.578 True Finance 60 Paula NaN 11/23/2005 2:01 PM 48866 4.271 False Distribution 64 Kathleen NaN 4/11/1990 6:46 PM 77834 18.771 False Business Development 69 Irene NaN 7/14/2015 4:31 PM 100863 4.382 True Finance 70 Todd NaN 6/10/2003 2:26 PM 84692 6.617 False Client Services 80 Gerald NaN 3/17/1995 12:50 AM 137126 15.602 True Sales 86 Annie NaN 9/29/2007 12:11 AM 103495 17.290 True Business Development 90 Janice NaN 8/21/1997 5:12 AM 91719 11.583 True Legal 91 James NaN 1/26/2005 11:00 PM 128771 8.309 False NaN 93 Virginia NaN 5/7/1994 5:58 PM 111858 1.601 True Legal 97 Laura NaN 7/19/2014 9:23 PM 140371 10.620 True Marketing 108 Russell NaN 5/5/1988 7:57 AM 133980 12.396 True Legal 121 Kathleen NaN 5/9/2016 8:55 AM 119735 18.740 False Product 143 Teresa NaN 1/28/2016 10:55 AM 140013 8.689 True Engineering 144 Nicole NaN 3/5/1982 2:28 PM 122717 12.452 False Sales 148 Patrick NaN 7/14/1991 2:24 AM 124488 14.837 True Sales 151 Brandon NaN 11/3/1997 8:17 PM 121333 15.295 False Business Development 153 Victor NaN 3/10/2011 8:40 PM 84546 10.489 True Finance 159 James NaN 11/22/1983 10:52 PM 68501 14.316 False Marketing 161 Marilyn NaN 8/22/1999 9:09 AM 103386 11.451 False Distribution 168 Peter NaN 9/3/1987 5:59 PM 38989 7.017 True Marketing 174 NaN NaN 9/18/2007 6:59 PM 40297 6.185 NaN Client Services 183 Ruth NaN 5/18/1999 5:56 AM 98233 2.518 True Distribution ... ... ... ... ... ... ... ... ... 769 Samuel NaN 10/7/2002 3:08 AM 141305 9.849 True Marketing 783 NaN NaN 4/15/1991 3:39 AM 132505 13.592 NaN Product 788 Michelle NaN 3/31/2012 6:28 AM 124441 16.353 False Business Development 792 Anne NaN 4/18/1996 11:57 PM 122762 9.564 False Distribution 795 Theresa NaN 10/7/1995 10:16 AM 42025 3.319 True Human Resources 815 Maria NaN 1/18/1986 8:36 PM 106562 4.000 False Human Resources 825 Robert NaN 12/4/2000 1:20 AM 69267 5.890 True Sales 826 NaN NaN 8/1/1988 1:35 AM 87103 5.665 NaN NaN 827 Jesse NaN 7/16/2014 2:24 AM 98811 7.487 False Legal 834 Carl NaN 2/11/1982 7:54 AM 49325 2.071 True Business Development 844 Maria NaN 6/19/1985 1:48 AM 148857 8.738 False Legal 847 Nicole NaN 5/2/1981 12:03 PM 41449 4.707 False Finance 855 Phillip NaN 10/20/2003 11:09 AM 89700 2.277 True NaN 856 Bonnie NaN 1/18/2006 6:52 PM 108946 12.211 False Finance 870 Cynthia NaN 11/19/1996 10:40 PM 107816 18.751 False Marketing 876 Terry NaN 9/11/1992 4:41 PM 41238 8.219 False Marketing 880 Robert NaN 5/25/2007 3:17 AM 90998 8.382 False Finance 882 Sara NaN 11/18/2014 2:47 PM 135990 14.344 True Distribution 895 Janice NaN 11/19/1991 6:02 PM 139791 16.968 False Business Development 897 Kenneth NaN 2/28/1994 10:10 AM 95296 10.146 False Finance 923 Irene NaN 2/28/1991 10:23 PM 135369 4.380 False Business Development 937 Aaron NaN 1/22/1986 7:39 PM 63126 18.424 False Client Services 938 Mark NaN 9/9/2006 12:27 PM 44836 2.657 False Client Services 939 Ralph NaN 7/28/1995 6:53 PM 70635 2.147 False Client Services 945 Gerald NaN 4/15/1989 12:44 PM 93712 17.426 True Distribution 961 Antonio NaN 6/18/1989 9:37 PM 103050 3.050 False Legal 972 Victor NaN 7/28/2006 2:49 PM 76381 11.159 True Sales 985 Stephen NaN 7/10/1983 8:10 PM 85668 1.909 False Legal 989 Justin NaN 2/10/1991 4:58 PM 38344 3.794 False Legal 995 Henry NaN 11/23/2014 6:09 AM 132483 16.655 False Distribution 145 rows \u00d7 8 columns # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe using dictionary df = pd . DataFrame ( dict ) # using notnull() function df . notnull () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 True True False 1 True True True 2 False True True 3 True False True .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b v1 v2 v3 v4 v5 10 R 1 1 3 2.0 11 R 1 1 3 3.0 12 R 1 1 3 NaN 13 R 1 1 3 5.0 14 R 1 1 4 1.0 15 R 1 1 4 2.0 16 R 1 1 4 3.0 17 R 1 1 4 4.0 18 R 1 1 4 5.0 19 R 1 1 5 1.0 20 R 1 1 5 2.0 21 R 1 1 5 3.0 22 R 1 1 5 4.0 23 R 1 1 5 5.0 24 L 1 2 1 1.0 # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # creating bool series True for NaN values bool_series = pd . notnull ( data [ \"Gender\" ]) # filtering data # displayind data only with Gender = Not NaN data [ bool_series ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 1 Thomas Male 3/31/1996 6:53 AM 61933 4.170 True NaN 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 7 NaN Female 7/20/2015 10:43 AM 45906 11.598 NaN Finance 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 25 NaN Male 10/8/2012 1:12 AM 37076 18.576 NaN Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal 30 Christina Female 8/6/2002 1:19 PM 118780 9.096 True Engineering 32 NaN Male 8/21/1998 2:27 PM 122340 6.417 NaN NaN 33 Jean Female 12/18/1993 9:07 AM 119082 16.180 False Business Development ... ... ... ... ... ... ... ... ... 966 Louis Male 8/16/2011 5:19 PM 93022 9.146 True Human Resources 967 Thomas Male 3/12/2016 3:10 PM 105681 19.572 False Engineering 968 Louise Female 3/27/1995 10:27 PM 43050 11.671 False Distribution 969 Linda Female 2/4/2010 8:49 PM 44486 17.308 True Engineering 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 855 rows \u00d7 8 columns # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling a missing value with # previous ones df . fillna ( method = 'pad' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Score Second Score Third Score 0 100.0 30.0 NaN 1 90.0 45.0 40.0 2 90.0 56.0 80.0 3 95.0 56.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling null value using fillna() function df . fillna ( method = 'bfill' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 40.0 1 90.0 45.0 40.0 2 95.0 56.0 80.0 3 95.0 NaN 98.0 # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # Printing the first 10 to 24 rows of # the data frame for visualization data [ 10 : 25 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois NaN 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua NaN 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # filling a null values using fillna() data [ \"Gender\" ] . fillna ( \"No Gender\" , inplace = True ) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 1 Thomas Male 3/31/1996 6:53 AM 61933 4.170 True NaN 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 7 NaN Female 7/20/2015 10:43 AM 45906 11.598 NaN Finance 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois No Gender 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua No Gender 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 25 NaN Male 10/8/2012 1:12 AM 37076 18.576 NaN Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 27 Scott No Gender 7/11/1991 6:58 PM 122367 5.218 False Legal 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal ... ... ... ... ... ... ... ... ... 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 972 Victor No Gender 7/28/2006 2:49 PM 76381 11.159 True Sales 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 985 Stephen No Gender 7/10/1983 8:10 PM 85668 1.909 False Legal 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 989 Justin No Gender 2/10/1991 4:58 PM 38344 3.794 False Legal 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 995 Henry No Gender 11/23/2014 6:09 AM 132483 16.655 False Distribution 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 1000 rows \u00d7 8 columns # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # Printing the first 10 to 24 rows of # the data frame for visualization data [ 10 : 25 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois NaN 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua NaN 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # will replace Nan value in dataframe with value -99 data . replace ( to_replace = np . nan , value = - 99 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 1 Thomas Male 3/31/1996 6:53 AM 61933 4.170 True -99 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 7 -99 Female 7/20/2015 10:43 AM 45906 11.598 -99 Finance 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True -99 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois -99 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua -99 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 -99 Male 6/14/2012 4:19 PM 125792 5.042 -99 -99 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 25 -99 Male 10/8/2012 1:12 AM 37076 18.576 -99 Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 27 Scott -99 7/11/1991 6:58 PM 122367 5.218 False Legal 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal ... ... ... ... ... ... ... ... ... 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 972 Victor -99 7/28/2006 2:49 PM 76381 11.159 True Sales 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 985 Stephen -99 7/10/1983 8:10 PM 85668 1.909 False Legal 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 989 Justin -99 2/10/1991 4:58 PM 38344 3.794 False Legal 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 995 Henry -99 11/23/2014 6:09 AM 132483 16.655 False Distribution 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 1000 rows \u00d7 8 columns # importing pandas as pd import pandas as pd # Creating the dataframe df = pd . DataFrame ({ \"A\" :[ 12 , 4 , 5 , None , 1 ], \"B\" :[ None , 2 , 54 , 3 , None ], \"C\" :[ 20 , 16 , None , 3 , 8 ], \"D\" :[ 14 , 3 , None , None , 6 ]}) # Print the dataframe df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 12.0 NaN 20.0 14.0 1 4.0 2.0 16.0 3.0 2 5.0 54.0 NaN NaN 3 NaN 3.0 3.0 NaN 4 1.0 NaN 8.0 6.0 # to interpolate the missing values df . interpolate ( method = 'linear' , limit_direction = 'forward' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 12.0 NaN 20.0 14.0 1 4.0 2.0 16.0 3.0 2 5.0 54.0 9.5 4.0 3 3.0 3.0 3.0 5.0 4 1.0 3.0 8.0 6.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , 40 , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52 NaN 1 90.0 NaN 40 NaN 2 NaN 45.0 80 NaN 3 95.0 56.0 98 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , 40 , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # using dropna() function df . dropna () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 3 95.0 56.0 98 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52.0 NaN 1 NaN NaN NaN NaN 2 NaN 45.0 80.0 NaN 3 95.0 56.0 98.0 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} df = pd . DataFrame ( dict ) # using dropna() function df . dropna ( how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52.0 NaN 2 NaN 45.0 80.0 NaN 3 95.0 56.0 98.0 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ 60 , 67 , 68 , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52.0 60 1 NaN NaN NaN 67 2 NaN 45.0 80.0 68 3 95.0 56.0 98.0 65 import pandas as pd import numpy as np dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ 60 , 67 , 68 , 65 ]} df = pd . DataFrame ( dict ) df . dropna ( axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } Fourth Score 0 60 1 67 2 68 3 65 import pandas as pd data = pd . read_csv ( \"employees.csv\" ) new_data = data . dropna ( axis = 0 , how = 'any' ) new_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal 30 Christina Female 8/6/2002 1:19 PM 118780 9.096 True Engineering 33 Jean Female 12/18/1993 9:07 AM 119082 16.180 False Business Development 34 Jerry Male 1/10/2004 12:56 PM 95734 19.096 False Client Services 35 Theresa Female 10/10/2006 1:12 AM 85182 16.675 False Sales 36 Rachel Female 2/16/2009 8:47 PM 142032 12.599 False Business Development 37 Linda Female 10/19/1981 8:49 PM 57427 9.557 True Client Services 38 Stephanie Female 9/13/1986 1:52 AM 36844 5.574 True Business Development 40 Michael Male 10/10/2008 11:25 AM 99283 2.665 True Distribution ... ... ... ... ... ... ... ... ... 966 Louis Male 8/16/2011 5:19 PM 93022 9.146 True Human Resources 967 Thomas Male 3/12/2016 3:10 PM 105681 19.572 False Engineering 968 Louise Female 3/27/1995 10:27 PM 43050 11.671 False Distribution 969 Linda Female 2/4/2010 8:49 PM 44486 17.308 True Engineering 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 764 rows \u00d7 8 columns","title":"MISSING VELLUES"},{"location":"Untitled8 (1)/#missing-velue","text":"cara mengisi data yang hilang adalah dengan menghitung jarak nya terlebih dahulu ,data yang hilang berkemungkinan akan bermasalah pada kemuadian hari. ada dua cara mengatasi missing velue dengan cara mengahapus data yang hilang ketika data tersebut dibawah 5% atau bisa dengan cara knn untuk mengisinyanya kembali ##### berikut langkah dari algoritma K-NN kita harus mengambil 2 tetangga terdekat dengan missing vellue hitung jarak tetangga tadi lalu urutkan dari yang terbesar ke yang terkecil ambil urutan yang terkecil lalu rata rata #### berikut ini cara mengatasi missing velue dengan python import pandas as pd import numpy as np dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} df = pd . DataFrame ( dict ) df . isnull () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 False False True 1 False False False 2 True False False 3 False True False # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # creating bool series True for NaN values bool_series = pd . isnull ( data [ \"Gender\" ]) # filtering data # displaying data only with Gender = NaN data [ bool_series ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 20 Lois NaN 4/22/1995 7:18 PM 64714 4.934 True Legal 22 Joshua NaN 3/8/2012 1:58 AM 90816 18.816 True Client Services 27 Scott NaN 7/11/1991 6:58 PM 122367 5.218 False Legal 31 Joyce NaN 2/20/2005 2:40 PM 88657 12.752 False Product 41 Christine NaN 6/28/2015 1:08 AM 66582 11.308 True Business Development 49 Chris NaN 1/24/1980 12:13 PM 113590 3.055 False Sales 51 NaN NaN 12/17/2011 8:29 AM 41126 14.009 NaN Sales 53 Alan NaN 3/3/2014 1:28 PM 40341 17.578 True Finance 60 Paula NaN 11/23/2005 2:01 PM 48866 4.271 False Distribution 64 Kathleen NaN 4/11/1990 6:46 PM 77834 18.771 False Business Development 69 Irene NaN 7/14/2015 4:31 PM 100863 4.382 True Finance 70 Todd NaN 6/10/2003 2:26 PM 84692 6.617 False Client Services 80 Gerald NaN 3/17/1995 12:50 AM 137126 15.602 True Sales 86 Annie NaN 9/29/2007 12:11 AM 103495 17.290 True Business Development 90 Janice NaN 8/21/1997 5:12 AM 91719 11.583 True Legal 91 James NaN 1/26/2005 11:00 PM 128771 8.309 False NaN 93 Virginia NaN 5/7/1994 5:58 PM 111858 1.601 True Legal 97 Laura NaN 7/19/2014 9:23 PM 140371 10.620 True Marketing 108 Russell NaN 5/5/1988 7:57 AM 133980 12.396 True Legal 121 Kathleen NaN 5/9/2016 8:55 AM 119735 18.740 False Product 143 Teresa NaN 1/28/2016 10:55 AM 140013 8.689 True Engineering 144 Nicole NaN 3/5/1982 2:28 PM 122717 12.452 False Sales 148 Patrick NaN 7/14/1991 2:24 AM 124488 14.837 True Sales 151 Brandon NaN 11/3/1997 8:17 PM 121333 15.295 False Business Development 153 Victor NaN 3/10/2011 8:40 PM 84546 10.489 True Finance 159 James NaN 11/22/1983 10:52 PM 68501 14.316 False Marketing 161 Marilyn NaN 8/22/1999 9:09 AM 103386 11.451 False Distribution 168 Peter NaN 9/3/1987 5:59 PM 38989 7.017 True Marketing 174 NaN NaN 9/18/2007 6:59 PM 40297 6.185 NaN Client Services 183 Ruth NaN 5/18/1999 5:56 AM 98233 2.518 True Distribution ... ... ... ... ... ... ... ... ... 769 Samuel NaN 10/7/2002 3:08 AM 141305 9.849 True Marketing 783 NaN NaN 4/15/1991 3:39 AM 132505 13.592 NaN Product 788 Michelle NaN 3/31/2012 6:28 AM 124441 16.353 False Business Development 792 Anne NaN 4/18/1996 11:57 PM 122762 9.564 False Distribution 795 Theresa NaN 10/7/1995 10:16 AM 42025 3.319 True Human Resources 815 Maria NaN 1/18/1986 8:36 PM 106562 4.000 False Human Resources 825 Robert NaN 12/4/2000 1:20 AM 69267 5.890 True Sales 826 NaN NaN 8/1/1988 1:35 AM 87103 5.665 NaN NaN 827 Jesse NaN 7/16/2014 2:24 AM 98811 7.487 False Legal 834 Carl NaN 2/11/1982 7:54 AM 49325 2.071 True Business Development 844 Maria NaN 6/19/1985 1:48 AM 148857 8.738 False Legal 847 Nicole NaN 5/2/1981 12:03 PM 41449 4.707 False Finance 855 Phillip NaN 10/20/2003 11:09 AM 89700 2.277 True NaN 856 Bonnie NaN 1/18/2006 6:52 PM 108946 12.211 False Finance 870 Cynthia NaN 11/19/1996 10:40 PM 107816 18.751 False Marketing 876 Terry NaN 9/11/1992 4:41 PM 41238 8.219 False Marketing 880 Robert NaN 5/25/2007 3:17 AM 90998 8.382 False Finance 882 Sara NaN 11/18/2014 2:47 PM 135990 14.344 True Distribution 895 Janice NaN 11/19/1991 6:02 PM 139791 16.968 False Business Development 897 Kenneth NaN 2/28/1994 10:10 AM 95296 10.146 False Finance 923 Irene NaN 2/28/1991 10:23 PM 135369 4.380 False Business Development 937 Aaron NaN 1/22/1986 7:39 PM 63126 18.424 False Client Services 938 Mark NaN 9/9/2006 12:27 PM 44836 2.657 False Client Services 939 Ralph NaN 7/28/1995 6:53 PM 70635 2.147 False Client Services 945 Gerald NaN 4/15/1989 12:44 PM 93712 17.426 True Distribution 961 Antonio NaN 6/18/1989 9:37 PM 103050 3.050 False Legal 972 Victor NaN 7/28/2006 2:49 PM 76381 11.159 True Sales 985 Stephen NaN 7/10/1983 8:10 PM 85668 1.909 False Legal 989 Justin NaN 2/10/1991 4:58 PM 38344 3.794 False Legal 995 Henry NaN 11/23/2014 6:09 AM 132483 16.655 False Distribution 145 rows \u00d7 8 columns # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe using dictionary df = pd . DataFrame ( dict ) # using notnull() function df . notnull () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 True True False 1 True True True 2 False True True 3 True False True .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b v1 v2 v3 v4 v5 10 R 1 1 3 2.0 11 R 1 1 3 3.0 12 R 1 1 3 NaN 13 R 1 1 3 5.0 14 R 1 1 4 1.0 15 R 1 1 4 2.0 16 R 1 1 4 3.0 17 R 1 1 4 4.0 18 R 1 1 4 5.0 19 R 1 1 5 1.0 20 R 1 1 5 2.0 21 R 1 1 5 3.0 22 R 1 1 5 4.0 23 R 1 1 5 5.0 24 L 1 2 1 1.0 # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # creating bool series True for NaN values bool_series = pd . notnull ( data [ \"Gender\" ]) # filtering data # displayind data only with Gender = Not NaN data [ bool_series ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 1 Thomas Male 3/31/1996 6:53 AM 61933 4.170 True NaN 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 7 NaN Female 7/20/2015 10:43 AM 45906 11.598 NaN Finance 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 25 NaN Male 10/8/2012 1:12 AM 37076 18.576 NaN Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal 30 Christina Female 8/6/2002 1:19 PM 118780 9.096 True Engineering 32 NaN Male 8/21/1998 2:27 PM 122340 6.417 NaN NaN 33 Jean Female 12/18/1993 9:07 AM 119082 16.180 False Business Development ... ... ... ... ... ... ... ... ... 966 Louis Male 8/16/2011 5:19 PM 93022 9.146 True Human Resources 967 Thomas Male 3/12/2016 3:10 PM 105681 19.572 False Engineering 968 Louise Female 3/27/1995 10:27 PM 43050 11.671 False Distribution 969 Linda Female 2/4/2010 8:49 PM 44486 17.308 True Engineering 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 855 rows \u00d7 8 columns # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling a missing value with # previous ones df . fillna ( method = 'pad' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Score Second Score Third Score 0 100.0 30.0 NaN 1 90.0 45.0 40.0 2 90.0 56.0 80.0 3 95.0 56.0 98.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling null value using fillna() function df . fillna ( method = 'bfill' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 30.0 40.0 1 90.0 45.0 40.0 2 95.0 56.0 80.0 3 95.0 NaN 98.0 # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # Printing the first 10 to 24 rows of # the data frame for visualization data [ 10 : 25 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois NaN 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua NaN 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # filling a null values using fillna() data [ \"Gender\" ] . fillna ( \"No Gender\" , inplace = True ) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 1 Thomas Male 3/31/1996 6:53 AM 61933 4.170 True NaN 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 7 NaN Female 7/20/2015 10:43 AM 45906 11.598 NaN Finance 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois No Gender 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua No Gender 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 25 NaN Male 10/8/2012 1:12 AM 37076 18.576 NaN Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 27 Scott No Gender 7/11/1991 6:58 PM 122367 5.218 False Legal 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal ... ... ... ... ... ... ... ... ... 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 972 Victor No Gender 7/28/2006 2:49 PM 76381 11.159 True Sales 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 985 Stephen No Gender 7/10/1983 8:10 PM 85668 1.909 False Legal 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 989 Justin No Gender 2/10/1991 4:58 PM 38344 3.794 False Legal 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 995 Henry No Gender 11/23/2014 6:09 AM 132483 16.655 False Distribution 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 1000 rows \u00d7 8 columns # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # Printing the first 10 to 24 rows of # the data frame for visualization data [ 10 : 25 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } \u200b First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True NaN 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois NaN 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua NaN 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 NaN Male 6/14/2012 4:19 PM 125792 5.042 NaN NaN 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services # importing pandas package import pandas as pd # making data frame from csv file data = pd . read_csv ( \"employees.csv\" ) # will replace Nan value in dataframe with value -99 data . replace ( to_replace = np . nan , value = - 99 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 1 Thomas Male 3/31/1996 6:53 AM 61933 4.170 True -99 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 7 -99 Female 7/20/2015 10:43 AM 45906 11.598 -99 Finance 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 10 Louise Female 8/12/1980 9:01 AM 63241 15.132 True -99 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 20 Lois -99 4/22/1995 7:18 PM 64714 4.934 True Legal 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 22 Joshua -99 3/8/2012 1:58 AM 90816 18.816 True Client Services 23 -99 Male 6/14/2012 4:19 PM 125792 5.042 -99 -99 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 25 -99 Male 10/8/2012 1:12 AM 37076 18.576 -99 Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 27 Scott -99 7/11/1991 6:58 PM 122367 5.218 False Legal 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal ... ... ... ... ... ... ... ... ... 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 972 Victor -99 7/28/2006 2:49 PM 76381 11.159 True Sales 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 985 Stephen -99 7/10/1983 8:10 PM 85668 1.909 False Legal 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 989 Justin -99 2/10/1991 4:58 PM 38344 3.794 False Legal 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 995 Henry -99 11/23/2014 6:09 AM 132483 16.655 False Distribution 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 1000 rows \u00d7 8 columns # importing pandas as pd import pandas as pd # Creating the dataframe df = pd . DataFrame ({ \"A\" :[ 12 , 4 , 5 , None , 1 ], \"B\" :[ None , 2 , 54 , 3 , None ], \"C\" :[ 20 , 16 , None , 3 , 8 ], \"D\" :[ 14 , 3 , None , None , 6 ]}) # Print the dataframe df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 12.0 NaN 20.0 14.0 1 4.0 2.0 16.0 3.0 2 5.0 54.0 NaN NaN 3 NaN 3.0 3.0 NaN 4 1.0 NaN 8.0 6.0 # to interpolate the missing values df . interpolate ( method = 'linear' , limit_direction = 'forward' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 12.0 NaN 20.0 14.0 1 4.0 2.0 16.0 3.0 2 5.0 54.0 9.5 4.0 3 3.0 3.0 3.0 5.0 4 1.0 3.0 8.0 6.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , 40 , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52 NaN 1 90.0 NaN 40 NaN 2 NaN 45.0 80 NaN 3 95.0 56.0 98 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , 40 , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # using dropna() function df . dropna () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 3 95.0 56.0 98 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52.0 NaN 1 NaN NaN NaN NaN 2 NaN 45.0 80.0 NaN 3 95.0 56.0 98.0 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ np . nan , np . nan , np . nan , 65 ]} df = pd . DataFrame ( dict ) # using dropna() function df . dropna ( how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52.0 NaN 2 NaN 45.0 80.0 NaN 3 95.0 56.0 98.0 65.0 # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ 60 , 67 , 68 , 65 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } First Score Second Score Third Score Fourth Score 0 100.0 30.0 52.0 60 1 NaN NaN NaN 67 2 NaN 45.0 80.0 68 3 95.0 56.0 98.0 65 import pandas as pd import numpy as np dict = { 'First Score' :[ 100 , np . nan , np . nan , 95 ], 'Second Score' : [ 30 , np . nan , 45 , 56 ], 'Third Score' :[ 52 , np . nan , 80 , 98 ], 'Fourth Score' :[ 60 , 67 , 68 , 65 ]} df = pd . DataFrame ( dict ) df . dropna ( axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } Fourth Score 0 60 1 67 2 68 3 65 import pandas as pd data = pd . read_csv ( \"employees.csv\" ) new_data = data . dropna ( axis = 0 , how = 'any' ) new_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } First Name Gender Start Date Last Login Time Salary Bonus % Senior Management Team 0 Douglas Male 8/6/1993 12:42 PM 97308 6.945 True Marketing 2 Maria Female 4/23/1993 11:17 AM 130590 11.858 False Finance 3 Jerry Male 3/4/2005 1:00 PM 138705 9.340 True Finance 4 Larry Male 1/24/1998 4:47 PM 101004 1.389 True Client Services 5 Dennis Male 4/18/1987 1:35 AM 115163 10.125 False Legal 6 Ruby Female 8/17/1987 4:20 PM 65476 10.012 True Product 8 Angela Female 11/22/2005 6:29 AM 95570 18.523 True Engineering 9 Frances Female 8/8/2002 6:51 AM 139852 7.524 True Business Development 11 Julie Female 10/26/1997 3:19 PM 102508 12.637 True Legal 12 Brandon Male 12/1/1980 1:08 AM 112807 17.492 True Human Resources 13 Gary Male 1/27/2008 11:40 PM 109831 5.831 False Sales 14 Kimberly Female 1/14/1999 7:13 AM 41426 14.543 True Finance 15 Lillian Female 6/5/2016 6:09 AM 59414 1.256 False Product 16 Jeremy Male 9/21/2010 5:56 AM 90370 7.369 False Human Resources 17 Shawn Male 12/7/1986 7:45 PM 111737 6.414 False Product 18 Diana Female 10/23/1981 10:27 AM 132940 19.082 False Client Services 19 Donna Female 7/22/2010 3:48 AM 81014 1.894 False Product 21 Matthew Male 9/5/1995 2:12 AM 100612 13.645 False Marketing 24 John Male 7/1/1992 10:08 PM 97950 13.873 False Client Services 26 Craig Male 2/27/2000 7:45 AM 37598 7.757 True Marketing 28 Terry Male 11/27/1981 6:30 PM 124008 13.464 True Client Services 29 Benjamin Male 1/26/2005 10:06 PM 79529 7.008 True Legal 30 Christina Female 8/6/2002 1:19 PM 118780 9.096 True Engineering 33 Jean Female 12/18/1993 9:07 AM 119082 16.180 False Business Development 34 Jerry Male 1/10/2004 12:56 PM 95734 19.096 False Client Services 35 Theresa Female 10/10/2006 1:12 AM 85182 16.675 False Sales 36 Rachel Female 2/16/2009 8:47 PM 142032 12.599 False Business Development 37 Linda Female 10/19/1981 8:49 PM 57427 9.557 True Client Services 38 Stephanie Female 9/13/1986 1:52 AM 36844 5.574 True Business Development 40 Michael Male 10/10/2008 11:25 AM 99283 2.665 True Distribution ... ... ... ... ... ... ... ... ... 966 Louis Male 8/16/2011 5:19 PM 93022 9.146 True Human Resources 967 Thomas Male 3/12/2016 3:10 PM 105681 19.572 False Engineering 968 Louise Female 3/27/1995 10:27 PM 43050 11.671 False Distribution 969 Linda Female 2/4/2010 8:49 PM 44486 17.308 True Engineering 970 Alice Female 9/3/1988 8:54 PM 63571 15.397 True Product 971 Patrick Male 12/30/2002 2:01 AM 75423 5.368 True Business Development 973 Russell Male 5/10/2013 11:08 PM 137359 11.105 False Business Development 974 Harry Male 8/30/2011 6:31 PM 67656 16.455 True Client Services 975 Susan Female 4/7/1995 10:05 PM 92436 12.467 False Sales 976 Denise Female 10/19/1992 5:42 AM 137954 4.195 True Legal 977 Sarah Female 12/4/1995 9:16 AM 124566 5.949 False Product 978 Sean Male 1/17/1983 2:23 PM 66146 11.178 False Human Resources 979 Ernest Male 7/20/2013 6:41 AM 142935 13.198 True Product 980 Kimberly Female 1/26/2013 12:57 AM 46233 8.862 True Engineering 981 James Male 1/15/1993 5:19 PM 148985 19.280 False Legal 982 Rose Female 4/6/1982 10:43 AM 91411 8.639 True Human Resources 983 John Male 12/23/1982 10:35 PM 146907 11.738 False Engineering 984 Maria Female 10/15/2011 4:53 PM 43455 13.040 False Engineering 986 Donna Female 11/26/1982 7:04 AM 82871 17.999 False Marketing 987 Gloria Female 12/8/2014 5:08 AM 136709 10.331 True Finance 988 Alice Female 10/5/2004 9:34 AM 47638 11.209 False Human Resources 990 Robin Female 7/24/1987 1:35 PM 100765 10.982 True Client Services 991 Rose Female 8/25/2002 5:12 AM 134505 11.051 True Marketing 992 Anthony Male 10/16/2011 8:35 AM 112769 11.625 True Finance 993 Tina Female 5/15/1997 3:53 PM 56450 19.040 True Engineering 994 George Male 6/21/2013 5:47 PM 98874 4.479 True Marketing 996 Phillip Male 1/31/1984 6:30 AM 42392 19.675 False Finance 997 Russell Male 5/20/2013 12:39 PM 96914 1.421 False Product 998 Larry Male 4/20/2013 4:45 PM 60500 11.985 False Business Development 999 Albert Male 5/15/2012 6:24 PM 129949 10.169 True Sales 764 rows \u00d7 8 columns","title":"MISSING VELUE"},{"location":"pengertian/","text":"STATISTIK DESKRIPTIF \u00b6 pengertian \u00b6 PENGERTIAN STATISTIK DEKRIPTIF ADALAH metode pengumpulan sebuah data data yang akan menghasilkan informasi yang berguna TIPE STATISTIK DESKRIPTIF \u00b6 MEAN(RATA-RATA) \u00b6 mean atau rata rata adalah sebuah nilai yang jumlah dari seluruah angka atau data dan di bagi banyak data . misal memiliki N data bisa di hitung dengan rumus sebagai berikut $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i={a_1+a_2+a_3+a_4+........+a_n \\over n} \\end{align} $$ keterangan: x=rata-rata a=nilai ke N n=banyak nilai atau data MEDIAN \u00b6 median merupakan nilai tengah dalam suatu data median disimbolkan Me .menghitung median mempunyai 2 metode yaitu ketika N atau jumlah data ganjil atau genap. berikut rumus median ; $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ ket: me =median atau nilai tengah n=banyak data MODUS \u00b6 MODUS ADALAH nilai yang sering muncul dalam himpunan data.brikut ini rumus mencari modus dalam himpunan data $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ ket; mo=nilai modus tb= tepi bawah b1=selisih frekuensi antara nilai mudus dengan elemen sebelumnya b2=selisih frekuensi antara nilai mudus dengan elemen sesudahnya p= panjang interval varian \u00b6 varian adalah penyebaran nilai dalam suatu data dari rata rata .berikut ini rumus dari varian dalam himpunan data $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ ket: x=rata rata Xi=rata rata dari semua titik data n= banyak dari anggota data standart deviasi \u00b6 Standar deviasi adalah ukuran dispersi himpunan data relatif pada rata-rata atau bisa juga akar kuadrat positif dari varian. berikut ini rumus standat deviasi: $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ skewness( kemiringan ) \u00b6 skewness( kemiringan ) adalah suatu bentuk derajat ketidaksimestrian suatu data.skewness juga di sebut angka atau bilang yang dapat menunjukan ketidakmiringan atau kemiringan suatu data. berikut rumus skewness $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ ket: Xi=titik data x=rata-rata n=jumlah titik distribusi o=standar deviasi QUARTILE \u00b6 quartile merupakan bagian nilai yang di bagi 4 sama rata atau di bagi 25% berikut rumus dari quartile $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ ket: q=nilai quarter n = banyak dari data Berikut penerapan statistik deskriptif di pyton \u00b6 sebelum itu buat data random melalui exsel lalu kita import file tadi seperti di bawah berikut import pandas as pd from scipy import stats df = pd . read_csv ( 'imam.csv' , sep = ';' ) data = { \"stats\" :[ 'Min' , 'Max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data ) tes . style . hide_index () berikut hasil setelah di jalankan stats ukuran baju ukuran sepatu ukaran celana umur Min 18 27 22 20 Max 30 42 36 40 Mean 23.76 34.452 28.812 29.828 Standart Deviasi 3.75 4.64 4.23 6.18 Variasi 14.05 21.53 17.93 38.18 Skewnes 0.08 0.02 0.08 -0 Quantile 1 21 30.75 25 24 Quantile 2 24 34 29 30 Quantile 3 27 39 32 35 Median 24 34 29 30 Modus 21 32 26 24 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"statistik deskriptif"},{"location":"pengertian/#statistik-deskriptif","text":"","title":"STATISTIK DESKRIPTIF"},{"location":"pengertian/#pengertian","text":"PENGERTIAN STATISTIK DEKRIPTIF ADALAH metode pengumpulan sebuah data data yang akan menghasilkan informasi yang berguna","title":"pengertian"},{"location":"pengertian/#tipe-statistik-deskriptif","text":"","title":"TIPE STATISTIK DESKRIPTIF"},{"location":"pengertian/#meanrata-rata","text":"mean atau rata rata adalah sebuah nilai yang jumlah dari seluruah angka atau data dan di bagi banyak data . misal memiliki N data bisa di hitung dengan rumus sebagai berikut $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i={a_1+a_2+a_3+a_4+........+a_n \\over n} \\end{align} $$ keterangan: x=rata-rata a=nilai ke N n=banyak nilai atau data","title":"MEAN(RATA-RATA)"},{"location":"pengertian/#median","text":"median merupakan nilai tengah dalam suatu data median disimbolkan Me .menghitung median mempunyai 2 metode yaitu ketika N atau jumlah data ganjil atau genap. berikut rumus median ; $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ ket: me =median atau nilai tengah n=banyak data","title":"MEDIAN"},{"location":"pengertian/#modus","text":"MODUS ADALAH nilai yang sering muncul dalam himpunan data.brikut ini rumus mencari modus dalam himpunan data $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ ket; mo=nilai modus tb= tepi bawah b1=selisih frekuensi antara nilai mudus dengan elemen sebelumnya b2=selisih frekuensi antara nilai mudus dengan elemen sesudahnya p= panjang interval","title":"MODUS"},{"location":"pengertian/#varian","text":"varian adalah penyebaran nilai dalam suatu data dari rata rata .berikut ini rumus dari varian dalam himpunan data $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ ket: x=rata rata Xi=rata rata dari semua titik data n= banyak dari anggota data","title":"varian"},{"location":"pengertian/#standart-deviasi","text":"Standar deviasi adalah ukuran dispersi himpunan data relatif pada rata-rata atau bisa juga akar kuadrat positif dari varian. berikut ini rumus standat deviasi: $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$","title":"standart deviasi"},{"location":"pengertian/#skewnesskemiringan","text":"skewness( kemiringan ) adalah suatu bentuk derajat ketidaksimestrian suatu data.skewness juga di sebut angka atau bilang yang dapat menunjukan ketidakmiringan atau kemiringan suatu data. berikut rumus skewness $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ ket: Xi=titik data x=rata-rata n=jumlah titik distribusi o=standar deviasi","title":"skewness(kemiringan)"},{"location":"pengertian/#quartile","text":"quartile merupakan bagian nilai yang di bagi 4 sama rata atau di bagi 25% berikut rumus dari quartile $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ ket: q=nilai quarter n = banyak dari data","title":"QUARTILE"},{"location":"pengertian/#berikut-penerapan-statistik-deskriptif-di-pyton","text":"sebelum itu buat data random melalui exsel lalu kita import file tadi seperti di bawah berikut import pandas as pd from scipy import stats df = pd . read_csv ( 'imam.csv' , sep = ';' ) data = { \"stats\" :[ 'Min' , 'Max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data ) tes . style . hide_index () berikut hasil setelah di jalankan stats ukuran baju ukuran sepatu ukaran celana umur Min 18 27 22 20 Max 30 42 36 40 Mean 23.76 34.452 28.812 29.828 Standart Deviasi 3.75 4.64 4.23 6.18 Variasi 14.05 21.53 17.93 38.18 Skewnes 0.08 0.02 0.08 -0 Quantile 1 21 30.75 25 24 Quantile 2 24 34 29 30 Quantile 3 27 39 32 35 Median 24 34 29 30 Modus 21 32 26 24 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Berikut penerapan statistik  deskriptif  di pyton"}]}